# -*- coding: utf-8 -*-
"""Projeto 7 - Churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12E9Vbb2X1ZQD89B9_Cm3QxWw_Vt7lPEn

# Churn

## Pacotes usados no projeto
"""

!pip install -q -U watermark

#import de bibliotecas

#salvar os modelos criados
import joblib

#manipulação dos dados
import pandas as pd
import numpy as np

#visualização de dados e geração de gráficos
import matplotlib.pyplot as plt
import seaborn as sns

#biblioteca para construção de modelos de machine learning
import sklearn
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

#configurações básicas dos gráficos
#para deixamos todos do mesmo tamanho e padrão
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = [6,5]

"""## Carregar os dados"""

#carregando os dados
df_churn = pd.read_csv('/content/drive/MyDrive/dataset.csv')

#verificação do tipo de objeto
type(df_churn)

#dimensões da tabela
df_churn.shape

#visualização do dataframe
df_churn.head()

#Informações dos dados
df_churn.info()

"""##EDA"""

#Analise exploratória de Dados
def eda(dados):

    for column in dados.columns:

        #Se a coluna for numérica
        if dados[column].dtype in ['int64', 'float64']:

            # histograma e boxplot
            fig, axes = plt.subplots(1,2)
            sns.histplot(dados[column], kde = True, ax = axes[0])
            sns.boxplot(x = 'Churn', y = column, data = dados, ax = axes[1])
            axes[0].set_title(f'Distribuição de {column}')
            axes[1].set_title(f'{column} vs Churn')
            plt.tight_layout()
            plt.show()

        #Se a coluna for categórica
        else:

            #Contagem da frequencia em relação ao churn
            fig, axes = plt.subplots(1,2)
            sns.countplot(x = column, data=dados, ax = axes[0])
            sns.countplot(x = column, hue = 'Churn', data = dados, ax =axes[1])
            axes[0].set_title(f'Distribuição de {column}')
            axes[1].set_title(f'{column} vs Churn')
            plt.tight_layout()
            plt.show()

#Realizando a EDA
eda(df_churn)

"""## Automatizar o processo de divisão em Dados de Treino e Teste"""

# Função para automatizar a divisão entre treino e teste
def split_dataset(data, target_column, test_size, random_state = 42):
    """
    Divide o dataset em conjuntos de treino e teste.

    Parâmetros:
    - data (DataFrame): O DataFrame completo.
    - target_column (str): O nome da coluna alvo (target).
    - test_size (float): A proporção do conjunto de teste.
    - random_state (int): Seed para a geração de números aleatórios (padrão é 42).

    Retorna:
    - X_train (DataFrame): Conjunto de treino para as variáveis independentes.
    - X_test (DataFrame): Conjunto de teste para as variáveis independentes.
    - y_train (Series): Conjunto de treino para a variável alvo.
    - y_test (Series): Conjunto de teste para a variável alvo.
    """

    # Dados de entrada
    X = data.drop(target_column, axis = 1)

    # Dados de saída
    y = data[target_column]

    # Divisão em treino e teste
    X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y,
                                                        test_size = test_size,
                                                        random_state = random_state)

    return X_train, X_test, y_train, y_test

#Uso da função anterior
valor_test_size = 0.3
X_train, X_test, y_train, y_test = split_dataset(df_churn, 'Churn', test_size = valor_test_size)

# Verificando o tamanho dos conjuntos de treino e teste
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""## Pré-processamento dos Dados

O pré processamento, especialmente a aplicação de técnicas de encondig e normalização dos dados, deve idealmente ser realizado antes da divisão do dataset em conjuntos de treino e teste. Isso evita o vazamento de informações do conjunto de teste para o conjunto de treino, o que pode acontecer se o pré-processamento for realizado antes da divisão.
"""

# Amostra de dadis
df_churn.head()

#Selecionando as várias categorias
categorical_colunas = df_churn.select_dtypes(include = ['object']).columns

categorical_colunas

"""### One-hot encoding"""

#Aplicando One-hot encoding separadamente aos conjuntos de treino e teste
encoder = OneHotEncoder(sparse_output = False)

"""Por padrão, o OneHotEncoder retorna uma matriz esparsa quando vc o utiliza para transformar dados. Uma matriz espaesa é uma maneira eficiente de armazenar dados com muitos zeros (valores não presentes). No entanto, se você definir sparse_output=False, o encoder retornará uma matriz densa (numpy_array) em vez de uma matriz esparsa. Uma matriz densa é mais fácil de trabalhar e entender, mas pode consumir mais memória se os dados forem muito grandes e a maioria dos valores for zero."""

#Treinando o enconder com o conjunto de treino e transformando
X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train[categorical_colunas]))

#Transformando e teste
X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_colunas]))

"""Renomear as colunas"""

#renomeando as colunas
X_train_encoded.columns = encoder.get_feature_names_out(categorical_colunas)
X_test_encoded.columns = encoder.get_feature_names_out(categorical_colunas)

"""Removendo as categorias originais e adicionando as codificadas"""

#removendo as categorias originais e colocando as novas
X_train_preprocessed = X_train.drop(categorical_colunas, axis = 1).reset_index(drop = True)
X_train_preprocessed = pd.concat([X_train_preprocessed, X_train_encoded], axis=1)

X_train_preprocessed.head()

"""Toda e qualquer transformação realizada nos dados de treino devem ser replicadas nos dados de tesye"""

#Removendo as colunas categóricas originais
X_test_preprocessed = X_test.drop(categorical_colunas, axis = 1).reset_index(drop = True)
X_test_preprocessed = pd.concat([X_test_preprocessed, X_test_encoded], axis = 1)

X_train_preprocessed.head()

X_test_preprocessed.head()

"""### Standar-Scaller"""

#variaves numericas
numerical_colunas = X_test_preprocessed.select_dtypes(include = ['int64', 'float64']).columns

#Criando o StandarSclaer
scaler = StandardScaler()

#aplicando o StandardScaler as variaveis numericas
X_train_preprocessed[numerical_colunas] = scaler.fit_transform(X_train_preprocessed[numerical_colunas])
X_test_preprocessed[numerical_colunas] = scaler.transform(X_test_preprocessed[numerical_colunas])

#vosualizaçãoa as primeiras linhas do treino e test preprocessed para ver se estão iguais
X_train_preprocessed.head()

X_test_preprocessed.head()

"""## Modelagem"""

#Criando o modelo random forest
modelo_churn = RandomForestClassifier(random_state = 42) #valor padrão

#treinando o modelo
modelo_churn.fit(X_train_preprocessed, y_train)

#Fazendo a previsão com os dados de teste
y_pred = modelo_churn.predict(X_test_preprocessed)

#Avaliando o modelo
acuracia = accuracy_score(y_test, y_pred)
relatorio_classificacao = classification_report(y_test, y_pred)

acuracia

print(relatorio_classificacao)

"""##Validação Cruzada"""

#Modelo RandomForest com validação cruzada
modelo_churn_VC = RandomForestClassifier(random_state = 42)

#Validação Cruzada
#Folds = 5
vc_scores = cross_val_score(modelo_churn_VC, X_train_preprocessed, y_train, cv = 5)

vc_scores

"""## Avalição do Modelo

### Optimização de Hiperparâmetros

A otimização de hiperparâmetros é uma etapa crucial no desenvolvimento de modelos de machine learning. Ao ajustar os hiperparâmetros de um modelo, pode-se:
"""

#Definindos os hiperparâmetros
param_gridS = {
    'n_estimators': [50, 100, 200],  #numero de árvores
    'max_depth': [None, 10, 20, 30], #profundidade máxima da árvore
    'min_samples_split': [2, 4, 6],  #número mínimo de amostras para dividir um nó
    'min_samples_leaf': [1, 2, 4]    #número mínimo de amostras exigidas em cada folha
}

#Criar modelo otimizado
modelo_otimizado = RandomForestClassifier(random_state = 42)

#configuração da busca em grade com validadção cruzada
grid_search = GridSearchCV(modelo_otimizado, param_gridS, cv =5, scoring = 'accuracy', n_jobs = -1)

grid_search.fit(X_train_preprocessed, y_train)

#melhores parâmetros
best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_params, best_score

"""## Versão Final do Modelo"""

#Criando o modelo RandomForest com os melhores hiperparametros
df_churn_final = RandomForestClassifier(n_estimators = best_params['n_estimators'],
                                        min_samples_split = best_params['min_samples_split'],
                                        min_samples_leaf= best_params['min_samples_leaf'],
                                        max_depth = best_params['max_depth'],
                                        random_state = 42
                                        )

#Treinando o modelo com o os melhores hiperparametros
df_churn_final.fit(X_train_preprocessed, y_train)

#realizando as previsoes
y_pred_final = df_churn_final.predict(X_test_preprocessed)

#Avaliando o modelo
acuracia_final = accuracy_score(y_test, y_pred_final)
relatorio_classificacao_final = classification_report(y_test, y_pred_final)

acuracia_final

print(relatorio_classificacao_final)

"""Agora devemos realizar o dumping dos modelos"""

#do Scaler
joblib.dump(scaler, '/content/drive/MyDrive/padronizador_churn.pkl')

#do modelo
joblib.dump(df_churn_final, '/content/drive/MyDrive/df_churn_final.pkl')

# Commented out IPython magic to ensure Python compatibility.
# %watermark -v -m

# Commented out IPython magic to ensure Python compatibility.
# %watermark --iversions